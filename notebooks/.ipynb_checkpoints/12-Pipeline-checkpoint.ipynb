{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from alphabet_detector import AlphabetDetector\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../data/comments1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_data)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    ad = AlphabetDetector()\n",
    "    res = x\n",
    "    for ch in string.punctuation:                                                                                                     \n",
    "        res = res.replace(ch, ' ')\n",
    "    res = ''.join([i for i in res if not i.isdigit()])\n",
    "    res = res.lower()\n",
    "    res = emoji_pattern.sub(r' ', res)\n",
    "    res = res.replace('\\n', ' ')\n",
    "    res = res.replace('\\t', ' ')\n",
    "    res = res.replace('\\ufeff', ' ')\n",
    "    res = res.replace('\\r\\n', '  ')\n",
    "    res = res.replace('\\xa0', ' ')\n",
    "    res = res.replace('ё', 'е')\n",
    "    res = re.sub(' +',' ', res)\n",
    "    if  not ad.only_alphabet_chars(res, \"CYRILLIC\"): \n",
    "        res = ''\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(clean)\n",
    "df = df[df.text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('../data/comments_clean1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove comments that were manualy labeld\n",
    "manual = pd.read_csv('../data/manual.csv')\n",
    "df = df[~df.id.isin(manual.id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    stemmer = SnowballStemmer(\"russian\", ignore_stopwords=True) \n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    if len(stemmed_word) <= 2:\n",
    "        return word\n",
    "    return stemmed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_bad_words = '../data/bad_words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = open(path_to_bad_words).read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dictionary = [] # array that will contain all bad words\n",
    "stemmed_dictionary = [] # array that will contain actual stems used for finding bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in bad_words:\n",
    "    report_dictionary.append(word)\n",
    "    stemmed_dictionary.append(stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_dictionary = [i for i in stemmed_dictionary if len(i) > 2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_dictionary = list(set(stemmed_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_dictionary.append('хуй') # nltk stemmer can not corectly stem word хуй\n",
    "stemmed_dictionary.append('хуе')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label coments with bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(x):\n",
    "    global stemmed_dictionary\n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    for bad_word in stemmed_dictionary:\n",
    "        for token in tokens:\n",
    "            if bad_word in token:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.text.apply(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative process of finding new bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word stemming and counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stemmed = dict()\n",
    "for sentence in df.text:\n",
    "    for token in nltk.word_tokenize(sentence):\n",
    "        if token not in all_stemmed:\n",
    "            all_stemmed[token] = stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = dict()\n",
    "for sentence in df.text:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        word = stem(token)\n",
    "        if word in all_counts:\n",
    "            all_counts[word] += 1\n",
    "        else:\n",
    "            all_counts[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood of words being in comments labeld as bad or labeld as not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(label):\n",
    "    labeld_part = dict()\n",
    "    part = df[df.label == label]\n",
    "    for index, row in part.iterrows():\n",
    "        sentence = row.text\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            stemed = stem(token)\n",
    "            if stemed != '':\n",
    "                if all_counts[stemed] >= 20:\n",
    "                    if stemed in labeld_part:\n",
    "                        labeld_part[stemed] += 1.0/len(part)\n",
    "                    else:\n",
    "                        labeld_part[stemed] = 1.0/len(part)\n",
    "    return labeld_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria functions for decision if word is bad or not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_distance(p_good, p_bad):\n",
    "    difference = dict()\n",
    "    for key in p_good:\n",
    "        if key in p_bad:\n",
    "            difference[key] =(p_bad[key] - p_good[key])/ np.maximum( p_bad[key], p_good[key]) \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logg_odds(p_good, p_bad):\n",
    "    ratio = dict()\n",
    "    for key in p_good:\n",
    "        if key in p_bad:\n",
    "            odds_good =  p_good[key]/(1 -  p_good[key])\n",
    "            odds_bad = p_bad[key]/(1- p_bad[key])\n",
    "            ratio[key] = np.log(odds_bad/odds_good)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting new bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstem(stem):\n",
    "    res = []\n",
    "    for key in all_stemmed:\n",
    "        if all_stemmed[key] == stem:\n",
    "            res.append(key)\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(x, rev = True):\n",
    "    return sorted(x.items(), key=operator.itemgetter(1), reverse=rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dictionary(word):\n",
    "    global stemmed_dictionary\n",
    "    if len(word) <= 2:\n",
    "        return False\n",
    "    #for key in stemmed_dictionary:\n",
    "        #if word in key:\n",
    "            #return False\n",
    "    stemmed_dictionary.append(word)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(df, report_dictionary, stemmed_dictionary):\n",
    "    iter = True\n",
    "    while(iter):\n",
    "        print('-----------------------------------')\n",
    "        print('labeld as abusive: ', len(df[df.label == True]))\n",
    "        #print('List of bad_words')\n",
    "        #print(report_dictionary)\n",
    "        p_bad = likelihood(True)\n",
    "        p_good = likelihood(False)\n",
    "        prob = relative_distance(p_good, p_bad)\n",
    "        print(sort(prob)[:10])\n",
    "        print('New bad words')\n",
    "        new_words = []\n",
    "        for key in prob:\n",
    "            if prob[key] > 0.9:\n",
    "                if update_dictionary(key):\n",
    "                    new_words += unstem(key)\n",
    "                    print( unstem(key))\n",
    "        report_dictionary += new_words\n",
    "        if len(new_words) == 0:\n",
    "            iter = False\n",
    "            break\n",
    "        #print('\\nNew dictionary')\n",
    "        #print(report_dictionary)\n",
    "        df['label'] =  df.text.apply(label)\n",
    "        print('relabeld as abusive: ', len(df[df.label == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "labeld as abusive:  13139\n",
      "[('чао', 0.9804790282154483), ('засад', 0.9795029796262208), ('закупа', 0.9627326902294923), ('колхоз', 0.9544510658360461), ('иностранц', 0.9471044635515374), ('дыряв', 0.935272567240697), ('овец', 0.935272567240697), ('звон', 0.9344095348039062), ('инфраструктур', 0.9316765987540689), ('хохлятск', 0.9276575751513672)]\n",
      "New bad words\n",
      "['лишение', 'лишенное', 'лишенным', 'лишения', 'лишенные', 'лишенный', 'лишенная', 'лишенными', 'лишень', 'лишении']\n",
      "['дрался', 'дрались', 'драли', 'драл']\n",
      "['дырявых', 'дырявые', 'дырявым', 'дырявого', 'дырявая', 'дырявое', 'дырявый']\n",
      "['типы', 'типам', 'типах', 'типа', 'типи', 'тип', 'типу', 'типов', 'типо']\n",
      "['звоните', 'звонил', 'звонили', 'звоним', 'звонишь', 'звонят', 'звонить', 'звонит', 'звони', 'звон', 'звонила']\n",
      "['закупаются', 'закупаем', 'закупает', 'закупать', 'закупается', 'закупают']\n",
      "['овец']\n",
      "['чао']\n",
      "['колхозы', 'колхозу', 'колхозах', 'колхоз', 'колхоза', 'колхозов', 'колхозам']\n",
      "['алкоголя', 'алкоголе', 'алкоголю', 'алкоголь']\n",
      "['инфраструктуры', 'инфраструктура', 'инфраструктур', 'инфраструктуру', 'инфраструктурой']\n",
      "['засада', 'засаде', 'засадят', 'засадили', 'засадить', 'засаду', 'засади']\n",
      "['дыру', 'дыре', 'дырой', 'дыры', 'дыра', 'дырою']\n",
      "['хохлятский', 'хохлятское', 'хохлятских', 'хохлятски', 'хохлятские', 'хохлятском', 'хохлятскую', 'хохлятскому', 'хохлятская']\n",
      "['иностранцев', 'иностранцам', 'иностранцами', 'иностранцем', 'иностранцы', 'иностранца']\n",
      "['чувствовалась', 'чувствовали', 'чувствовать']\n",
      "['наркоману', 'наркоманов', 'наркоманию', 'наркоманами', 'наркоманят', 'наркомания', 'наркомании', 'наркоманом', 'наркоманам']\n",
      "['опущенная', 'опущенных', 'опущенное', 'опущеный', 'опущеная', 'опущенным', 'опущеное', 'опущения', 'опущенному', 'опущенны', 'опущенность', 'опущенный', 'опущенные', 'опущений', 'опущенного']\n",
      "relabeld as abusive:  13565\n",
      "-----------------------------------\n",
      "labeld as abusive:  13565\n",
      "[('младш', 0.9142134387351778), ('занят', 0.9106389986824769), ('нравствен', 0.8990746338060915), ('треб', 0.8962826410937599), ('наркотик', 0.8951497584541064), ('“', 0.8927667984189724), ('финлянд', 0.8927667984189723), ('покрышк', 0.8927667984189722), ('увелич', 0.8871229457041814), ('пив', 0.8871229457041814)]\n",
      "New bad words\n",
      "['младше', 'младшими', 'младший', 'младшая', 'младшему', 'младшим', 'младшего', 'младшие']\n",
      "['занятиях', 'занятий', 'занятия', 'занята', 'занятая', 'занятие', 'занято', 'занятость', 'занятья', 'занятием', 'заняты']\n",
      "relabeld as abusive:  13592\n",
      "-----------------------------------\n",
      "labeld as abusive:  13592\n",
      "[('заня', 0.9139692385594027), ('нравствен', 0.89878733948165), ('треб', 0.8959873990779661), ('наркотик', 0.8948512915726032), ('покрышк', 0.8924615481992532), ('финлянд', 0.892461548199253), ('“', 0.892461548199253), ('увелич', 0.8868016296834245), ('пив', 0.8868016296834245), ('тявка', 0.8868016296834245)]\n",
      "New bad words\n",
      "['занялась', 'занялся', 'заняла', 'заняли', 'занялись', 'заняв', 'занять', 'заняться']\n",
      "relabeld as abusive:  13602\n",
      "-----------------------------------\n",
      "labeld as abusive:  13602\n",
      "[('нравствен', 0.8986808095404808), ('треб', 0.8958779220994897), ('наркотик', 0.8947406188003882), ('финлянд', 0.8923483601367609), ('покрышк', 0.8923483601367608), ('“', 0.8923483601367608), ('увелич', 0.8866824843544852), ('пив', 0.8866824843544852), ('тявка', 0.8866824843544852), ('подруг', 0.8803870668186232)]\n",
      "New bad words\n"
     ]
    }
   ],
   "source": [
    "iterate(df, report_dictionary, stemmed_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual['evaluation'] = manual.text.apply(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6294117647058823\n",
      "precision:  0.7445482866043613\n",
      "recall:  0.6907514450867052\n",
      "f1:  0.7166416791604198\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for index, row in manual.iterrows():\n",
    "    if row.label == True and row.evaluation == True:\n",
    "        tp += 1\n",
    "    if row.label == False and row.evaluation == False:\n",
    "        tn += 1\n",
    "    if row.label == False and row.evaluation == True:\n",
    "        fp += 1\n",
    "    if row.label == True and row.evaluation == False:\n",
    "        #print(row.text)\n",
    "        fn += 1\n",
    "accuracy = (tp + fp)/(tp + fp + fn + fp)\n",
    "precision = tp/(tp + fp)\n",
    "recall = tp/(tp + fn)\n",
    "f1  = 2 * (precision * recall)/(precision + recall)\n",
    "print('accuracy: ', accuracy)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
