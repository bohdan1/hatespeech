{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from alphabet_detector import AlphabetDetector\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import xml.etree.ElementTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../data/comments1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_data)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    ad = AlphabetDetector()\n",
    "    res = x\n",
    "    for ch in string.punctuation:                                                                                                     \n",
    "        res = res.replace(ch, ' ')\n",
    "    res = ''.join([i for i in res if not i.isdigit()])\n",
    "    res = res.lower()\n",
    "    res = emoji_pattern.sub(r' ', res)\n",
    "    res = res.replace('\\n', ' ')\n",
    "    res = res.replace('\\t', ' ')\n",
    "    res = res.replace('\\ufeff', ' ')\n",
    "    res = res.replace('\\r\\n', '  ')\n",
    "    res = res.replace('\\xa0', ' ')\n",
    "    res = res.replace('ё', 'е')\n",
    "    res = re.sub(' +',' ', res)\n",
    "    if  not ad.only_alphabet_chars(res, \"CYRILLIC\"): \n",
    "        res = ''\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(clean)\n",
    "df = df[df.text != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('../data/comments_clean1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove comments that were manualy labeld\n",
    "manual = pd.read_csv('../data/manual.csv')\n",
    "df = df[~df.id.isin(manual.id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    stemmer = SnowballStemmer(\"russian\", ignore_stopwords=True) \n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    if len(stemmed_word) <= 2:\n",
    "        return word\n",
    "    return stemmed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_bad_words = '../data/bad_words.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = open(path_to_bad_words).read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dictionary = [] # array that will contain all bad words\n",
    "stemmed_dictionary = [] # array that will contain actual stems used for finding bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in bad_words:\n",
    "    report_dictionary.append(word)\n",
    "    stemmed_dictionary.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_dictionary = [i for i in stemmed_dictionary if len(i) > 2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_dictionary.append('хуй') # nltk stemmer can not corectly stem word хуй\n",
    "stemmed_dictionary.append('хуе')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_dictionary = list(set(stemmed_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['серька',\n",
       " 'заговнять',\n",
       " 'пидар',\n",
       " 'хуeм',\n",
       " 'подъебнуться',\n",
       " 'ебаное',\n",
       " 'хуеплет',\n",
       " 'мудил',\n",
       " 'блябу',\n",
       " 'нехира',\n",
       " 'задрота',\n",
       " 'сикель',\n",
       " 'похую',\n",
       " 'разъебать',\n",
       " 'похеру',\n",
       " 'наговнять',\n",
       " 'хуевато',\n",
       " 'заё6',\n",
       " 'проеб',\n",
       " 'ибанамат',\n",
       " 'пердануть',\n",
       " 'залупаться',\n",
       " 'бляд',\n",
       " 'жопу',\n",
       " 'заебастый',\n",
       " 'насрать',\n",
       " 'вафлёр',\n",
       " 'xуе',\n",
       " 'хyё',\n",
       " 'проебанка',\n",
       " 'говняк',\n",
       " 'ебaть',\n",
       " 'педрик',\n",
       " 'мокрощёлка',\n",
       " 'сцуль',\n",
       " 'ебучий',\n",
       " 'мандой',\n",
       " 'манду',\n",
       " 'Нехуй',\n",
       " 'курвятник',\n",
       " 'разъебай',\n",
       " 'гнид',\n",
       " 'долбоящер',\n",
       " 'бля',\n",
       " 'потаскуха',\n",
       " 'выебен',\n",
       " 'охуенно',\n",
       " 'писька',\n",
       " 'уебк',\n",
       " 'усраться',\n",
       " 'eбёт',\n",
       " 'уебки',\n",
       " 'наебнуть',\n",
       " 'херовый',\n",
       " 'пездо',\n",
       " 'заебистое',\n",
       " 'ябывает',\n",
       " 'ебучее',\n",
       " 'ебла',\n",
       " 'трахаеб',\n",
       " 'блябуду',\n",
       " 'уёбищное',\n",
       " 'подонки',\n",
       " 'сирать',\n",
       " 'изговняться',\n",
       " 'надристать',\n",
       " 'ебло',\n",
       " 'подонок',\n",
       " 'педрила',\n",
       " 'бляди',\n",
       " 'охуяньчик',\n",
       " 'хуюл',\n",
       " 'уебище',\n",
       " 'ебyч',\n",
       " 'хуякать',\n",
       " 'долбоёб',\n",
       " 'ебошить',\n",
       " 'злоебучий',\n",
       " 'захуячить',\n",
       " 'пидоры',\n",
       " 'xуй',\n",
       " 'въебусь',\n",
       " 'жопа',\n",
       " 'минет',\n",
       " 'сцыха',\n",
       " 'ебал',\n",
       " 'говнолинк',\n",
       " 'пи3д',\n",
       " 'припизднутый',\n",
       " 'ебат',\n",
       " 'выебать',\n",
       " 'заебал',\n",
       " 'мандавошки',\n",
       " 'сучка',\n",
       " 'пидорка',\n",
       " 'дрочун',\n",
       " 'бздло',\n",
       " 'охуячить',\n",
       " 'пердунец',\n",
       " 'пиздить',\n",
       " 'въеб',\n",
       " 'переёбок',\n",
       " 'ниипет',\n",
       " 'ебанамать',\n",
       " 'блядище',\n",
       " 'залупить',\n",
       " 'пиздобол',\n",
       " 'шлюшка',\n",
       " 'педрилло',\n",
       " 'спиздил',\n",
       " 'блядина',\n",
       " 'пиzдец',\n",
       " 'остоебенить',\n",
       " 'выпердеть',\n",
       " 'ебуче',\n",
       " 'обьебос',\n",
       " 'пиздунья',\n",
       " 'пидарье',\n",
       " 'заеб',\n",
       " 'говенка',\n",
       " 'ниипацца',\n",
       " 'дрочить',\n",
       " 'ебут',\n",
       " 'уёбки',\n",
       " 'похуй',\n",
       " 'eблантий',\n",
       " 'пиздатое',\n",
       " 'мокрощелка',\n",
       " 'залуп',\n",
       " 'поебень',\n",
       " 'хуйло',\n",
       " 'пиздатый',\n",
       " 'блядунья',\n",
       " 'хуясе',\n",
       " 'выеб',\n",
       " 'пиздюк',\n",
       " 'уебок',\n",
       " 'обосранец',\n",
       " 'пидарасы',\n",
       " 'пиздоболы',\n",
       " 'раздолбай',\n",
       " 'долбоеб',\n",
       " 'разхуячить',\n",
       " 'ёбн',\n",
       " 'хуем',\n",
       " 'гомосекам',\n",
       " 'малафья',\n",
       " 'распиздяй',\n",
       " 'блять',\n",
       " 'муде',\n",
       " 'мудня',\n",
       " 'ебаная',\n",
       " 'мандеть',\n",
       " 'e6aль',\n",
       " 'похрену',\n",
       " 'ебин',\n",
       " 'пиздануться',\n",
       " 'сестроеб',\n",
       " 'наебет',\n",
       " 'охуительный',\n",
       " 'въебывать',\n",
       " 'бздюшка',\n",
       " 'ебашить',\n",
       " 'охуячивать',\n",
       " 'пиздище',\n",
       " 'пёрднуть',\n",
       " 'пездишь',\n",
       " 'срун',\n",
       " 'хуй',\n",
       " 'выебнулся',\n",
       " 'ебнуть',\n",
       " 'ебучим',\n",
       " 'пердильник',\n",
       " 'пиздячить',\n",
       " 'хуерыло',\n",
       " 'шалавой',\n",
       " 'пизда',\n",
       " 'нахер',\n",
       " 'уебать',\n",
       " 'педерас',\n",
       " 'педрило',\n",
       " 'хуякнуть',\n",
       " 'урюк',\n",
       " 'взьебывать',\n",
       " 'блядун',\n",
       " 'перднуть',\n",
       " 'е6ал',\n",
       " 'обьебать',\n",
       " 'охуевающий',\n",
       " 'бздюшко',\n",
       " 'говназия',\n",
       " 'обдристаться',\n",
       " 'падонок',\n",
       " 'невротебучий',\n",
       " 'ебанат',\n",
       " 'ебануть',\n",
       " 'бздеть',\n",
       " '6ля',\n",
       " 'ебтвоюмать',\n",
       " 'ебун',\n",
       " 'паскуда',\n",
       " 'пидары',\n",
       " 'манда',\n",
       " 'бздецы',\n",
       " 'мандища',\n",
       " 'выебываться',\n",
       " 'говноед',\n",
       " 'ебануться',\n",
       " 'дрочелло',\n",
       " 'ебуч',\n",
       " 'мудак',\n",
       " 'засирать',\n",
       " 'хер',\n",
       " 'выблядыш',\n",
       " 'муди',\n",
       " 'блядуны',\n",
       " 'еблыст',\n",
       " 'изговнять',\n",
       " 'похерили',\n",
       " 'ахуел',\n",
       " 'обосцаться',\n",
       " 'взьебка',\n",
       " 'доебываться',\n",
       " 'похерил',\n",
       " 'настопиздить',\n",
       " 'суходрочка',\n",
       " 'ебущ',\n",
       " 'нахрен',\n",
       " 'пиздарваньчик',\n",
       " 'ебня',\n",
       " 'ебанный',\n",
       " 'задрачивать',\n",
       " 'ебаный',\n",
       " 'нихера',\n",
       " 'писюшка',\n",
       " 'ебись',\n",
       " 'пропиздячить',\n",
       " 'уёбища',\n",
       " 'страхопиздище',\n",
       " 'залупиться',\n",
       " 'хуйня',\n",
       " 'пиздорванец',\n",
       " 'засеря',\n",
       " 'хуёк',\n",
       " 'хую',\n",
       " 'пиздёныш',\n",
       " 'мудила',\n",
       " 'припиздень',\n",
       " 'сучко',\n",
       " 'пиздюлина',\n",
       " 'гнида',\n",
       " 'пердь',\n",
       " 'мандень',\n",
       " '6лядь',\n",
       " 'чмошник',\n",
       " 'припиздюлина',\n",
       " 'елдак',\n",
       " 'дрисня',\n",
       " 'пи3ду',\n",
       " 'сучара',\n",
       " 'пизденка',\n",
       " 'соси',\n",
       " 'хуетень',\n",
       " 'гавно',\n",
       " 'пердуха',\n",
       " 'поёбываает',\n",
       " 'хyй',\n",
       " 'заёбистое',\n",
       " 'однохуйственно',\n",
       " 'мудозвон',\n",
       " 'бздех',\n",
       " 'говнецо',\n",
       " 'заёбистый',\n",
       " 'хуесоска',\n",
       " 'заебашить',\n",
       " 'дристануть',\n",
       " 'eбaл',\n",
       " 'похуистка',\n",
       " 'ебальник',\n",
       " 'стерва',\n",
       " 'ебан',\n",
       " 'обосрать',\n",
       " 'въебался',\n",
       " 'хамло',\n",
       " 'секель',\n",
       " 'говнище',\n",
       " 'пердунья',\n",
       " 'говнюха',\n",
       " 'распиздяйство',\n",
       " 'сцать',\n",
       " 'дристун',\n",
       " 'распиздай',\n",
       " 'бздение',\n",
       " 'хуек',\n",
       " 'пездит',\n",
       " 'сцышь',\n",
       " 'отпиздить',\n",
       " 'eбaть',\n",
       " 'пизды',\n",
       " 'чмырь',\n",
       " 'заебистые',\n",
       " 'лошок',\n",
       " 'петух',\n",
       " 'пиздонутые',\n",
       " 'подговнять',\n",
       " 'херовина',\n",
       " 'пиздища',\n",
       " 'ебарь',\n",
       " 'бздун',\n",
       " 'злоебучая',\n",
       " 'заебать',\n",
       " 'охуительно',\n",
       " 'хуерик',\n",
       " 'пердунина',\n",
       " 'eбать',\n",
       " 'придурок',\n",
       " 'пропизделся',\n",
       " 'потаскушка',\n",
       " 'уёбок',\n",
       " 'херня',\n",
       " 'xую',\n",
       " '6лять',\n",
       " 'мандавошка',\n",
       " 'хуенький',\n",
       " 'хуище',\n",
       " 'ебанныйврот',\n",
       " 'шалава',\n",
       " 'блядь',\n",
       " 'ёбат',\n",
       " 'замудохаться',\n",
       " 'хуйрик',\n",
       " 'пиздит',\n",
       " 'гомосек',\n",
       " 'гамно',\n",
       " 'ебёна',\n",
       " 'изъебнуться',\n",
       " 'высраться',\n",
       " 'обосцать',\n",
       " 'педик',\n",
       " 'поскуда',\n",
       " 'набздел',\n",
       " 'пидорок',\n",
       " 'серун',\n",
       " 'злоебучее',\n",
       " 'похрен',\n",
       " 'мудеть',\n",
       " 'гавнюк',\n",
       " 'хуля',\n",
       " 'дрочка',\n",
       " 'напиздили',\n",
       " 'уёбище',\n",
       " 'похерила',\n",
       " 'вьебен',\n",
       " 'быдло',\n",
       " 'дрочена',\n",
       " 'пиздорванка',\n",
       " 'eбyч',\n",
       " 'хуёвенький',\n",
       " 'заёбистые',\n",
       " 'пропиздеть',\n",
       " 'заебанец',\n",
       " 'заебись',\n",
       " 'гондон',\n",
       " 'говночист',\n",
       " 'нахуй',\n",
       " 'мудaк',\n",
       " 'заеба',\n",
       " 'минетчица',\n",
       " 'блядюга',\n",
       " 'млять',\n",
       " 'пиздиться',\n",
       " 'падонки',\n",
       " 'проебать',\n",
       " 'лярва',\n",
       " 'объебос',\n",
       " 'хуяра',\n",
       " 'промандеть',\n",
       " 'заябестая',\n",
       " 'охуеньчик',\n",
       " 'пёрнуть',\n",
       " 'пиздят',\n",
       " 'ебнуться',\n",
       " 'хуя',\n",
       " 'заебаться',\n",
       " 'отпороть',\n",
       " 'мандюк',\n",
       " 'пидор',\n",
       " 'пиздишь',\n",
       " 'педрилы',\n",
       " 'подъебнуть',\n",
       " 'ипаццо',\n",
       " 'бзднуть',\n",
       " 'сцуконах',\n",
       " 'пездят',\n",
       " 'наебать',\n",
       " 'сыкун',\n",
       " 'архипиздрит',\n",
       " 'похуист',\n",
       " 'посрать',\n",
       " 'напиздел',\n",
       " 'распиздеться',\n",
       " 'пернуть',\n",
       " 'поебать',\n",
       " 'говнядина',\n",
       " 'взъебка',\n",
       " 'нехрен',\n",
       " 'конча',\n",
       " 'сцуки',\n",
       " 'задристать',\n",
       " 'ипаться',\n",
       " 'гандон',\n",
       " 'разъеб',\n",
       " 'хитрожопый',\n",
       " 'ибонех',\n",
       " 'ебливый',\n",
       " 'засрун',\n",
       " 'нахуйник',\n",
       " 'пездень',\n",
       " 'хуета',\n",
       " 'ебанько',\n",
       " 'наебнуться',\n",
       " 'нехуйственно',\n",
       " 'спиздили',\n",
       " 'пизду',\n",
       " 'похер',\n",
       " 'дрочистый',\n",
       " 'пиздеть',\n",
       " 'лох',\n",
       " 'курва',\n",
       " 'опездал',\n",
       " 'блядовать',\n",
       " 'дристать',\n",
       " 'спиздит',\n",
       " 'хуевый',\n",
       " 'выблядок',\n",
       " 'пердеж',\n",
       " 'дрочила',\n",
       " 'сучонок',\n",
       " 'блядство',\n",
       " 'въебенн',\n",
       " 'ебля',\n",
       " 'зае6',\n",
       " 'ебическая',\n",
       " 'лошара',\n",
       " 'хуячить',\n",
       " 'ёбаная',\n",
       " 'залупа',\n",
       " 'пиздюля',\n",
       " 'баран',\n",
       " 'ебырь',\n",
       " 'хуепромышленник',\n",
       " 'падла',\n",
       " 'пиздюга',\n",
       " 'xyя',\n",
       " 'остопиздеть',\n",
       " 'говняный',\n",
       " 'урод',\n",
       " 'гниды',\n",
       " 'пердение',\n",
       " 'хуенч',\n",
       " 'сволочь',\n",
       " 'ебская',\n",
       " 'шараёбиться',\n",
       " 'напиздели',\n",
       " 'ебик',\n",
       " 'хуею',\n",
       " 'мудель',\n",
       " 'целка',\n",
       " 'проблядь',\n",
       " 'хуел',\n",
       " 'сраный',\n",
       " 'отмудохать',\n",
       " 'набздеть',\n",
       " 'заебываться',\n",
       " 'шлюха',\n",
       " 'хуевина',\n",
       " 'хуяк',\n",
       " 'ебать',\n",
       " 'ебатория',\n",
       " 'заёб',\n",
       " 'злоеб',\n",
       " 'пердеть',\n",
       " 'хуёвый',\n",
       " 'пизденыш',\n",
       " 'бздицы',\n",
       " 'говенный',\n",
       " 'обсирать',\n",
       " 'съебаться',\n",
       " 'мандей',\n",
       " 'пидорасы',\n",
       " 'спиздила',\n",
       " 'вафел',\n",
       " 'сраку',\n",
       " 'мудаг',\n",
       " 'дрист',\n",
       " 'уроды',\n",
       " 'минетчик',\n",
       " 'пидрас',\n",
       " 'охуеть',\n",
       " 'пиздун',\n",
       " 'промудеть',\n",
       " 'сцыкун',\n",
       " 'пиздуй',\n",
       " 'пиздоватая',\n",
       " 'нихуя',\n",
       " 'елда',\n",
       " 'гандоны',\n",
       " 'гавнючка',\n",
       " 'мудak',\n",
       " 'ебало',\n",
       " 'мудоеб',\n",
       " 'ебки',\n",
       " 'сговнять',\n",
       " 'говно',\n",
       " 'пиздобратия',\n",
       " 'наебывать',\n",
       " 'хуйком',\n",
       " 'мудоклюй',\n",
       " 'пи3де',\n",
       " 'отпиздячить',\n",
       " 'пипец',\n",
       " 'е6ут',\n",
       " 'сцука',\n",
       " 'мразь',\n",
       " 'мандавошек',\n",
       " 'сволота',\n",
       " 'бздит',\n",
       " 'пиздоватый',\n",
       " 'ушлепок',\n",
       " 'ебаться',\n",
       " 'суки',\n",
       " 'разъеба',\n",
       " 'ниибацо',\n",
       " 'охуел',\n",
       " 'пидарас',\n",
       " 'сука',\n",
       " 'пиздато',\n",
       " 'пиздануть',\n",
       " 'мудистый',\n",
       " 'сучий',\n",
       " 'заебастая',\n",
       " 'опизденивающе',\n",
       " 'невъебенно',\n",
       " 'ёбaн',\n",
       " 'ипать',\n",
       " 'бздунья',\n",
       " 'бздюха',\n",
       " 'блядки',\n",
       " 'пидарaс',\n",
       " 'ублюдок',\n",
       " 'xyй',\n",
       " 'засерать',\n",
       " 'долбаёб',\n",
       " 'отъебись',\n",
       " 'лошарa',\n",
       " 'хуё',\n",
       " 'елдачить',\n",
       " 'пердун',\n",
       " 'ахуеть',\n",
       " 'дрочилка',\n",
       " 'сранье',\n",
       " 'ссышь',\n",
       " 'ебский',\n",
       " 'хуе',\n",
       " 'выебон',\n",
       " 'надрочить',\n",
       " 'трахаёб',\n",
       " 'трахае6',\n",
       " 'сучье',\n",
       " 'шлюхой',\n",
       " 'хуево',\n",
       " 'охуевать',\n",
       " 'сирывать',\n",
       " 'очкун',\n",
       " 'еблище',\n",
       " 'запиздячить',\n",
       " 'ниипаццо',\n",
       " 'хyйня',\n",
       " 'заебошить',\n",
       " 'ёбаную',\n",
       " 'писюн',\n",
       " 'спиздел',\n",
       " 'лошары',\n",
       " 'распроеть',\n",
       " 'опизде',\n",
       " 'спиздеть',\n",
       " 'хуеплёт',\n",
       " 'напиздело',\n",
       " 'еблан',\n",
       " 'xyёв',\n",
       " 'ебанический',\n",
       " 'заебистый',\n",
       " 'ебец',\n",
       " 'пиздострадатель',\n",
       " 'чмо',\n",
       " 'никуя',\n",
       " 'дристуха',\n",
       " 'срака',\n",
       " 'спиздить',\n",
       " 'уебищное',\n",
       " 'говнять',\n",
       " 'хуи',\n",
       " 'засерун',\n",
       " 'охуевательский',\n",
       " 'сцание',\n",
       " 'трахатель',\n",
       " 'ебёт',\n",
       " 'говешка',\n",
       " 'хуесос',\n",
       " 'пиздолиз',\n",
       " 'приебаться',\n",
       " 'выссаться',\n",
       " 'говнюк',\n",
       " 'хуеныш',\n",
       " 'хитровыебанный',\n",
       " 'пиздец',\n",
       " 'долбаёбы',\n",
       " 'срать',\n",
       " 'ебет',\n",
       " 'херовато',\n",
       " 'ссака',\n",
       " 'пробзделся']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label coments with bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(x):\n",
    "    global stemmed_dictionary\n",
    "    tokens = nltk.word_tokenize(x)\n",
    "    for bad_word in stemmed_dictionary:\n",
    "        for token in tokens:\n",
    "            if bad_word in token:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.text.apply(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative process of finding new bad words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word stemming and counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stemmed = dict()\n",
    "for sentence in df.text:\n",
    "    for token in nltk.word_tokenize(sentence):\n",
    "        if token not in all_stemmed:\n",
    "            all_stemmed[token] = stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = dict()\n",
    "for sentence in df.text:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        word = stem(token)\n",
    "        if word in all_counts:\n",
    "            all_counts[word] += 1\n",
    "        else:\n",
    "            all_counts[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = dict()\n",
    "for index, row in df.iterrows():\n",
    "    tokens = nltk.word_tokenize(row.text)\n",
    "    for token in tokens:\n",
    "        word = stem(token)\n",
    "        if word in word_to_id:\n",
    "            word_to_id[word].append(row.video_id)\n",
    "        else:\n",
    "            word_to_id[word] = [row.video_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in word_to_id:\n",
    "    word_to_id[key] = len(list(set(word_to_id[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood of words being in comments labeld as bad or labeld as not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(label):\n",
    "    labeld_part = dict()\n",
    "    part = df[df.label == label]\n",
    "    for index, row in part.iterrows():\n",
    "        sentence = row.text\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            stemed = stem(token)\n",
    "            if stemed != '':\n",
    "                if all_counts[stemed] >= 20 and word_to_id[stemed] > 30:\n",
    "                    if stemed in labeld_part:\n",
    "                        labeld_part[stemed] += 1.0/len(part)\n",
    "                    else:\n",
    "                        labeld_part[stemed] = 1.0/len(part)\n",
    "    return labeld_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria functions for decision if word is bad or not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_distance(p_good, p_bad):\n",
    "    difference = dict()\n",
    "    for key in p_good:\n",
    "        if key in p_bad:\n",
    "            difference[key] =(p_bad[key] - p_good[key])/ np.maximum( p_bad[key], p_good[key]) \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logg_odds(p_good, p_bad):\n",
    "    ratio = dict()\n",
    "    for key in p_good:\n",
    "        if key in p_bad:\n",
    "            odds_good =  p_good[key]/(1 -  p_good[key])\n",
    "            odds_bad = p_bad[key]/(1- p_bad[key])\n",
    "            ratio[key] = np.log(odds_bad/odds_good)\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(df_bad):\n",
    "    pairs = dict()\n",
    "    for sentence in df_bad.text:\n",
    "        selection = []\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            word = stem(token)\n",
    "            if word in stemmed_dictionary and all_counts[word] >= 20:\n",
    "                selection.append(word)\n",
    "        for bad_word in selection:\n",
    "            for token in nltk.word_tokenize(sentence):\n",
    "                word = stem(token)\n",
    "                if bad_word != word and word != '' and all_counts[word] >= 30:\n",
    "                    key = bad_word + '-' + word\n",
    "                    if key in pairs:\n",
    "                        pairs[key] += 1\n",
    "                    else:\n",
    "                        pairs[key] = 1\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi():\n",
    "    bad_likelihood = likelihood(True)\n",
    "    pairs = make_pairs(df[df.label == True])\n",
    "    result = dict()\n",
    "    for key in pairs:\n",
    "        words = key.split('-')\n",
    "        pxy = float(pairs[key]/len(df[df.label == True]))\n",
    "        px = bad_likelihood[words[0]]\n",
    "        py = bad_likelihood[words[1]]\n",
    "        result[key] = np.log(pxy/(px*py))\n",
    "    return result\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining with corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-00b93ae77882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../corpus/opcorpora.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \"\"\"\n\u001b[1;32m   1183\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    594\u001b[0m                     \u001b[0;31m# It can be used to parse the whole source without feeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m                     \u001b[0;31m# it with chunks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_whole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = xml.etree.ElementTree.parse('../corpus/opcorpora.xml').getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_rus_corpus(corpus, word):\n",
    "    for lemma in corpus.iter('lemma'):\n",
    "        found = False\n",
    "        for forms in lemma.getchildren():\n",
    "            if forms.attrib['t'] == word:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting new bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstem(stem):\n",
    "    res = []\n",
    "    for key in all_stemmed:\n",
    "        if all_stemmed[key] == stem:\n",
    "            res.append(key)\n",
    "    return res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(x, rev = True):\n",
    "    return sorted(x.items(), key=operator.itemgetter(1), reverse=rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dictionary(word):\n",
    "    global stemmed_dictionary\n",
    "    if len(word) <= 2:\n",
    "        return False\n",
    "    #for key in stemmed_dictionary:\n",
    "        #if word in key:\n",
    "            #return False\n",
    "    stemmed_dictionary.append(word)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(df, report_dictionary, stemmed_dictionary):\n",
    "    iter = True\n",
    "    while(iter):\n",
    "        print('-----------------------------------')\n",
    "        print('labeld as abusive: ', len(df[df.label == True]))\n",
    "        #print('List of bad_words')\n",
    "        #print(report_dictionary)\n",
    "        p_bad = likelihood(True)\n",
    "        p_good = likelihood(False)\n",
    "        prob = relative_distance(p_good, p_bad)\n",
    "        print(sort(prob)[:10])\n",
    "        print('New bad words')\n",
    "        new_words = []\n",
    "        for key in prob:\n",
    "            if prob[key] > 0.80:\n",
    "                if update_dictionary(key):\n",
    "                    new_words += unstem(key)\n",
    "                    print(key)\n",
    "        report_dictionary += new_words\n",
    "        if len(new_words) == 0:\n",
    "            iter = False\n",
    "            break\n",
    "        #print('\\nNew dictionary')\n",
    "        #print(report_dictionary)\n",
    "        df['label'] =  df.text.apply(label)\n",
    "        print('relabeld as abusive: ', len(df[df.label == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "labeld as abusive:  11963\n",
      "[('кацапск', 0.8394765510538), ('хул', 0.8267684446788923), ('рот', 0.8188492878642133), ('пошел', 0.8167441955463868), ('падл', 0.8090917553604121), ('жрат', 0.8008138910941969), ('«', 0.7937719579510615), ('»', 0.7936277425370413), ('бендеровск', 0.792978773173951), ('безмозгл', 0.7878797281782357)]\n",
      "New bad words\n",
      "жрат\n",
      "пошел\n",
      "рот\n",
      "падл\n",
      "хул\n",
      "кацапск\n",
      "relabeld as abusive:  13680\n",
      "-----------------------------------\n",
      "labeld as abusive:  13680\n",
      "[('»', 0.8615304528891211), ('«', 0.8610291707636849), ('соглашен', 0.8424619309575061), ('услов', 0.8366081695915202), ('бизнес', 0.8123070590656513), ('продава', 0.8087801492278328), ('промышлен', 0.8035199363627323), ('коррупц', 0.8007210750573578), ('—', 0.7985138717747653), ('отставк', 0.7930565010211029)]\n",
      "New bad words\n",
      "продава\n",
      "коррупц\n",
      "услов\n",
      "промышлен\n",
      "бизнес\n",
      "соглашен\n",
      "relabeld as abusive:  13940\n",
      "-----------------------------------\n",
      "labeld as abusive:  13940\n",
      "[('«', 0.8726491017400115), ('»', 0.8715512491688048), ('ассоциац', 0.826729256198347), ('кита', 0.8263552521260028), ('—', 0.8251080354475758), ('ресурс', 0.8182060808249295), ('средн', 0.8180948238364503), ('шли', 0.8175895316804406), ('экономическ', 0.817494476720399), ('производств', 0.8079889807162531)]\n",
      "New bad words\n",
      "столиц\n",
      "производств\n",
      "товар\n",
      "отставк\n",
      "кита\n",
      "шли\n",
      "ресурс\n",
      "вышл\n",
      "ассоциац\n",
      "экономическ\n",
      "средн\n",
      "предприят\n",
      "relabeld as abusive:  14708\n",
      "-----------------------------------\n",
      "labeld as abusive:  14708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-572b1959daa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmed_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-187-b0ccc8e78ad0>\u001b[0m in \u001b[0;36miterate\u001b[0;34m(df, report_dictionary, stemmed_dictionary)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#print(report_dictionary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mp_bad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mp_good\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_good\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_bad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-47c9ab8ce770>\u001b[0m in \u001b[0;36mlikelihood\u001b[0;34m(label)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mstemed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstemed\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mall_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstemed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstemed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-61b8a35fd6fc>\u001b[0m in \u001b[0;36mstem\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"russian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstemmed_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, ignore_stopwords)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The language '{0}' is not supported.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mstemmerclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Stemmer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmerclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ignore_stopwords)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterate(df, report_dictionary, stemmed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id['майдаун']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual['evaluation'] = manual.text.apply(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5851449275362319\n",
      "precision:  0.651702786377709\n",
      "recall:  0.6437308868501529\n",
      "f1:  0.6476923076923076\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for index, row in manual.iterrows():\n",
    "    if row.label == True and row.evaluation == True:\n",
    "        tp += 1\n",
    "    if row.label == False and row.evaluation == False:\n",
    "        tn += 1\n",
    "    if row.label == False and row.evaluation == True:\n",
    "        fp += 1\n",
    "    if row.label == True and row.evaluation == False:\n",
    "        #print(row.text)\n",
    "        fn += 1\n",
    "accuracy = (tp + fp)/(tp + fp + fn + fp)\n",
    "precision = tp/(tp + fp)\n",
    "recall = tp/(tp + fn)\n",
    "f1  = 2 * (precision * recall)/(precision + recall)\n",
    "print('accuracy: ', accuracy)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
